# TODO: Most parameters should be in a default PPO config
name: "subspace_ppo"

defaults:
  - gradient_subspace@algorithm.gradient_subspace: random_subspace
  - optimizer@algorithm.policy_kwargs.optimizer: adam
  - env_wrappers: []

algorithm:
  _target_: rl_subspace_optimization.subspace_ppo.SubspacePPO
  gradient_subspace: ???
  warmup_steps_orig_space: 10000
  project_before_differentiating: true
  policy: "MlpPolicy"
  policy_kwargs:
    optimizer: ???
  learning_rate: 3e-4
  device: "auto"
  tensorboard_log: tensorboard
  update_trajectory_logging_interval: null

training:
  steps: 1000000
  n_envs: 1
