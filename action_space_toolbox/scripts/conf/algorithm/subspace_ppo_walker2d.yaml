# TODO: Most parameters should be in a default PPO config
name: "subspace_ppo"

defaults:
  - gradient_subspace@algorithm.gradient_subspace: random_subspace
  - optimizer@algorithm.policy_kwargs.optimizer: adam
  - env_wrappers: []

algorithm:
  _target_: rl_subspace_optimization.subspace_ppo.SubspacePPO
  gradient_subspace: ???
  warmup_steps_orig_space: 10000
  project_before_differentiating: true
  policy: "MlpPolicy"
  policy_kwargs:
    optimizer: ???
  device: "auto"
  batch_size: 32
  n_steps: 512
  gamma: 0.99
  learning_rate: 5.05041e-05
  ent_coef: 0.000585045
  clip_range: 0.1
  n_epochs: 20
  gae_lambda: 0.95
  max_grad_norm: 1
  vf_coef: 0.871923
  tensorboard_log: tensorboard
  update_trajectory_logging_interval: null

training:
  steps: 5_000_000
  n_envs: 1
