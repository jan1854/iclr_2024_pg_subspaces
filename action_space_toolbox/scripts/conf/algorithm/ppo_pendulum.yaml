# TODO: Most parameters should be in a default PPO config
name: "ppo"

defaults:
  - optimizer@algorithm.policy_kwargs.optimizer: adam

algorithm:
  _target_: stable_baselines3.PPO
  policy: "MlpPolicy"
  policy_kwargs:
    optimizer: ???
  learning_rate: 0.003
  n_steps: 1024
  batch_size: 64
  n_epochs: 10
  gamma: 0.9
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: true
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: true
  sde_sample_freq: 4
  target_kl: null
  tensorboard_log: tensorboard
  create_eval_env: false
  policy_kwargs: null
  verbose: 0
  device: "auto"

training:
  steps: 600000
  n_envs: 4