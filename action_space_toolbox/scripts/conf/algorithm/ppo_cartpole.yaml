# TODO: Most parameters should be in a default PPO config
name: "ppo"

defaults:
  - optimizer@algorithm.policy_kwargs.optimizer: adam
  - env_wrappers: []

algorithm:
  _target_: stable_baselines3.PPO
  policy: "MlpPolicy"
  policy_kwargs:
    optimizer: ???
  n_steps: 32
  batch_size: 256
  gae_lambda: 0.8
  gamma: 0.98
  n_epochs: 20
  ent_coef: 0.0
  learning_rate:
    _target_: action_space_toolbox.training.schedules.LinearSchedule
    start: 0.001
    end: 0.0
  clip_range:
    _target_: action_space_toolbox.training.schedules.LinearSchedule
    start: 0.2
    end: 0.0
  device: "auto"
  tensorboard_log: tensorboard

training:
  steps: 100_000
  n_envs: 8