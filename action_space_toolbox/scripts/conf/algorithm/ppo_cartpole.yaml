# TODO: Most parameters should be in a default PPO config
name: "ppo"

algorithm:
  _target_: stable_baselines3.PPO
  n_envs: 8
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  n_steps: 32
  batch_size: 256
  gae_lambda: 0.8
  gamma: 0.98
  n_epochs: 20
  ent_coef: 0.0
  learning_rate: lin_0.001
  clip_range: lin_0.2
  device: "auto"
  tensorboard_log: tensorboard

training:
  steps: 600000