# TODO: Most parameters should be in a default PPO config
name: "ppo"

defaults:
  - env_wrappers:
      - vec_normalize
  - activation_fn@algorithm.policy_kwargs.activation_fn: relu

algorithm:
  _target_: stable_baselines3.PPO
  policy: "MlpPolicy"
  device: ${device}
  policy_kwargs:
    log_std_init: -2
    ortho_init: false
    activation_fn: ???
    net_arch:
      pi:
        - 256
        - 256
      vf:
        - 256
        - 256
  batch_size: 256
  n_steps: 512
  gamma: 0.95
  learning_rate: 3.56987e-05
  ent_coef: 0.00238306
  clip_range: 0.3
  n_epochs: 5
  gae_lambda: 0.9
  max_grad_norm: 2
  vf_coef: 0.431892

training:
  steps: 10_000_000
  n_envs: 1